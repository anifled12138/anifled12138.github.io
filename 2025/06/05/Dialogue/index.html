<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Position Debiasing Fine-Tuning for Causal Perception in Long-Term Dialogue | To be or not to be</title><meta name="author" content="Anifled"><meta name="copyright" content="Anifled"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="收录于 IJCAI 2024                Abstract 本文提出了一种 因果感知的长对话框架（Causal Perception long-term Dialogue framework, CPD）用于解决在长对话中llm的位置偏差（Position Bias，指在模型在处理输入序列时，倾向于优先关注某些特定位置的信息，而忽视其他位置的重要内容，在长文本或对话任务中尤为">
<meta property="og:type" content="website">
<meta property="og:title" content="Position Debiasing Fine-Tuning for Causal Perception in Long-Term Dialogue">
<meta property="og:url" content="http://anifled12138.github.io/2025/06/05/Dialogue/index.html">
<meta property="og:site_name" content="To be or not to be">
<meta property="og:description" content="收录于 IJCAI 2024                Abstract 本文提出了一种 因果感知的长对话框架（Causal Perception long-term Dialogue framework, CPD）用于解决在长对话中llm的位置偏差（Position Bias，指在模型在处理输入序列时，倾向于优先关注某些特定位置的信息，而忽视其他位置的重要内容，在长文本或对话任务中尤为">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://anifled12138.github.io/img/Hazel.jpg">
<meta property="article:published_time" content="2025-06-05T04:28:02.000Z">
<meta property="article:modified_time" content="2025-07-11T12:51:00.450Z">
<meta property="article:author" content="Anifled">
<meta property="article:tag" content="causal debias">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://anifled12138.github.io/img/Hazel.jpg"><link rel="shortcut icon" href="/img/bitbug_favicon.ico"><link rel="canonical" href="http://anifled12138.github.io/2025/06/05/Dialogue/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'medium_zoom',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Position Debiasing Fine-Tuning for Causal Perception in Long-Term Dialogue',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'position'
}</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/1749020983551.gif" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/categories/Java/"><span> Java</span></a></div><div class="menus_item"><a class="site-page" href="/categories/Python/"><span> Python</span></a></div><div class="menus_item"><a class="site-page" href="/categories/Papers/"><span> Papers</span></a></div><div class="menus_item"><a class="site-page" href="/categories/LeetCode/"><span> LeetCode</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-home-page" id="page-header" style="background-color: rgb(0,0,0);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">To be or not to be</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/categories/Java/"><span> Java</span></a></div><div class="menus_item"><a class="site-page" href="/categories/Python/"><span> Python</span></a></div><div class="menus_item"><a class="site-page" href="/categories/Papers/"><span> Papers</span></a></div><div class="menus_item"><a class="site-page" href="/categories/LeetCode/"><span> LeetCode</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="page-site-info"><h1 id="site-title">Position Debiasing Fine-Tuning for Causal Perception in Long-Term Dialogue</h1></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div>
  <span style="float: left;">收录于 <strong><em>IJCAI 2024</em></strong></span>
  <a href="https://arxiv.org/pdf/2406.02002.pdf" target="_blank" style="float: right;">
    <img src="/images/icons8-pdf-40.png" alt="PDF" width="40">
  </a>
  <div style="clear: both;"></div>
</div>
<h1>Abstract</h1>
<p>本文提出了一种 <em><strong>因果感知的长对话框架</strong></em>（Causal Perception long-term Dialogue framework, CPD）用于解决在长对话中llm的<em><strong>位置偏差</strong></em>（Position Bias，指在模型在处理输入序列时，倾向于优先关注某些特定位置的信息，而忽视其他位置的重要内容，在长文本或对话任务中尤为明显）。这一偏差导致模型过度地关注虚假的位置相关性，而非真正的因果相关的话语。</p>
<img src="/images/image-20250605132928174.png" alt="image-20250605132928174" style="zoom: 67%;">
<h2 id="主要工作：">主要工作：</h2>
<ul>
<li>提出一种基于因果扰动的话语提取方法，结合局部位置感知机制，能够有效避免LLM的位置信息干扰</li>
<li>提出一种因果感知微调策略，显著缓解模型的“位置偏差”问题，提升模型在对话中捕捉因果关系的能力</li>
</ul>
<p><strong>baseline</strong>：</p>
<p><strong>dataset</strong>：CGDIALOG、 ESConv、MSC</p>
<p><strong>基础模型</strong>：Llama2-7B-chat、Qwen-14B-chat</p>
<h1>Method</h1>
<p><strong>处理效应</strong>（Treatment Effect,TE）用于量化某个变量对结果的影响，通常通过条件独立性假设和反事实推理进行评估。对于发言u<sub>i</sub>，用二元处理设定来评估其对回复结果的处理效应。</p>
<p>对发言u<sub>i</sub>的TE定义如下：<br>
$$<br>
TE(u_i) = f(D)-f\bigl(D \setminus u_i\bigr)<br>
$$<br>
D\u<sub> i</sub>表示对话中去除该发言后的情形，函数f(·)表示llm生成正确回复的困扰度（perplexity，[Horgan, 1995]），为了排除去除发言后其他变量改变可能带来的影响，使用一些长度相近但是无实际意义的发言（如hello、thank you等）替换u<sub>i</sub>，构造反事实处理状态。</p>
<h2 id="因果识别">因果识别</h2>
<p>模型的因果识别能力与因果相关发言在对话中的位置紧密相关。</p>
<p>采用归一化处理效应来衡量发言与回复之间的因果相关性：<br>
$$<br>
TE_{\text{reg}}(u_i) = \frac{f(D) - f(D \setminus u_i)}{f(D)}<br>
$$<br>
<img src="/images/image-20250605171318756.png" alt="image-20250605171318756" style="zoom: 67%;"></p>
<p>a：原始模型，b:微调后，c：移除位置嵌入，d：本文方法</p>
<p><strong>观察图（a）（b）发现：</strong></p>
<ul>
<li>大语言模型只能识别出对话中最后1到2轮的因果关系；</li>
<li>无论语句是否相关，模型对最后几轮语句的扰动始终表现出更高的敏感性；</li>
<li>尽管对领域数据进行微调增强了模型对因果语句的敏感度，但在更长的对话历史中准确区分仍然具有挑战性。</li>
</ul>
<p>这说明llm具备区分相关与无关语句的潜力，一处所有位置嵌入如（c）可以消除位置信息对llm的影响，但是会破坏语义信息，使模型无法识别相关语句。</p>
<h2 id="因果相关语句提取">因果相关语句提取</h2>
<p>作者为了在保持语义信息与消除position bias之间获得平衡，提出了一种基于<em><strong>句子级</strong></em>的局部位置感知方法，应用于语义模型的每一层。作者将<strong>位置信息限制在句子内部，在句间的注意力只是用语义相关性</strong>。为确保方法适用于不同位置编码方式的模型，直接<strong>修改注意力矩阵</strong>。</p>
<p>当输入单词位于同一语句中时，模型使用包含位置编码的注意力$A_{t,s}^{\mathrm{pe}}$ ，而当不在同义句时，注意力不包含位置编码  $\tilde{A}_{t,s}^{\mathrm{pe}}$， 公式如下：</p>
<img src="/images/image-20250608220706643.png" alt="image-20250608220706643" style="zoom:50%;">
<p>x<sub>t</sub>与x<sub>s</sub>分别是第t和s个输入词，$m_A^{\mathrm{pe}}$和$\tilde{m}_A^{\mathrm{pe}}$分别是注意力矩阵$m^{pe}$与$\tilde{m}^{pe}$分别是注意力矩阵$A^{pe}$与$\tilde{A}^{pe}$的均值，用于平衡两种注意力矩阵之间的差异。经过微调后达到图（d）中的性能。</p>
<p>为了从对话中<strong>提取最小的因果相关话语集合</strong>，作者分别测量每个话语的TE，表示为$[\mathrm{TE}(u_1),\ \mathrm{TE}(u_2),\ \ldots,\ \mathrm{TE}(u_{|D|})]$，其中$|D|$表示对话轮数，采用k-means聚类算法划分因果相关集合S，两个初始聚类中心分别设置为输入数据的最小值与中位数。</p>
<p>附录B.1实验结果表明，<em><strong>话语之间在条件下是独立的，基于单句扰动和聚类的因果提取发给发不会陷入局部最优解。</strong></em></p>
<p>在<strong>CGDIALOG</strong>测试集上验证了该方法的有效性，达到了**88.6%**的精度。作者在两个长期对话数据集中提取相关话语，并计算因果相关话语的位置分布Q，q<sub>i</sub>表示与回答相聚i轮的因果相关话语的频率。</p>
<p>结论：***因果相关话语在位置分布上存在严重不平衡，这可能是造成模型位置偏置的原因之一。</p>
<p><img src="/images/image-20250609002142774.png" alt="image-20250609002142774"></p>
<h2 id="因果感知微调">因果感知微调</h2>
<p>LLMs的微调采用指令微调凡是，即将指令与对话D拼接后输入模型生成响应R：<br>
$$<br>
p® = p(R \mid \text{instruction}, D) = \prod_{t} p(r_{t+1} \mid \text{instruction}, D, r_1, r_2, \ldots, r_t)<br>
$$<br>
微调过程中，目标是使模型从数据中学习领域知识的同时，对因果关联保持敏感。</p>
<p>损失函数包含两部分：</p>
<ul>
<li>预测损失（Prediction Loss）</li>
<li>因果感知损失（Causal Perception Loss）</li>
</ul>
<p>$$<br>
L = L_{\mathrm{Pred}} + \overbrace{\alpha L_{\mathrm{IRM}} + \beta L_{\mathrm{MTE}}}^{\text{causal perception}}<br>
$$</p>
<p><strong>预测损失：</strong><br>
$$<br>
L_{\mathrm{Pred}} = -\sum_{r_t \in R} \log p(r_t \mid \text{instruction}, D)<br>
$$<br>
<strong>因果感知损失</strong>由两部分组成：</p>
<ol>
<li>
<p>不动风险最小化（IBM）损失L<sub>IRM</sub>：鼓励模型在不同环境（由扰动非因果相关话语构造）中保持输出一致性。构造的反事实对话D/u<sub>i</sub>替换掉u<sub>i</sub>∈S。替换项来自其他对话中的非因果相关话语。<br>
$$<br>
L_{\mathrm{IRM}} = \sum_{r_i \in R} \mathrm{KL}\left( \mathrm{sg}\big(p(r_i \mid D)\big) | p(r_i \mid D \setminus u_i) \right),u_i \in S<br>
$$</p>
</li>
<li>
<p>最大化处理效应（MTE）损失L<sub>MTE</sub>：因果相关话语u<sub>i</sub>∈C被替换时，<strong>期望模型生成的输出与原输出差异最大</strong>。</p>
</li>
</ol>
<p>$$<br>
L_{\mathrm{MTE}} = -\sum_{r_i \in R} \mathrm{KL}\left( \mathrm{sg}(p(r_i \mid D)) | p(r_i \mid D \setminus u_i) \right),u_i \in C<br>
$$</p>
<h3 id="采样策略">采样策略</h3>
<p>为应对相关话语分布不均，作者在低频因果相关位置上进行更更多扰动。对每个对话多次执行IRM和MTE任务，执行次数n：<br>
$$<br>
n = \left\lfloor \frac{|C|}{\sum\limits_{u_i \in C} (q|D| - i)} \right\rfloor<br>
$$<br>
[·]表示向下取整，q|D|-i 是第i轮在数据集中为因果相关话语的频率。</p>
<h1>实验</h1>
<h2 id="dataset">dataset</h2>
<p>CGDIALOG、 ESConv、MSC</p>
<h2 id="baseline">baseline</h2>
<ol>
<li>
<p><strong>原始及微调后的大语言模型（LLM）</strong>：这些模型在开放域对话任务中表现出色。通过在特定任务领域进行微调，模型可以显著提升其任务表现。</p>
</li>
<li>
<p><strong>长期对话方法</strong>：</p>
<p><strong>RSM</strong> 持续对长期对话进行摘要，并将摘要作为外部记忆输入，以缓解LLM在处理长期对话时的遗忘问题。</p>
<p><strong>CONSTRAIN</strong> 假设在对话中除了最后一句以外，仅有一句历史话语与当前回复相关。它通过一个训练好的语言模型在对话历史中检索相关语句，并将其与最后一句拼接，作为输入进行回复生成。</p>
</li>
<li>
<p><strong>位置去偏方法</strong>：</p>
<p><strong>RPP</strong>  将位置扰动扩展到句子级别，以打破训练数据中的位置分布不均问题。</p>
<p><strong>ZOE</strong>  同时拟合金标准回复与原模型生成的次优回复，从而在微调模型与原模型之间实现一致性。</p>
</li>
</ol>
<h3 id="评估指标">评估指标</h3>
<ol>
<li><strong>词重叠度</strong>：使用 BLEU-n（n=1, 2）来评估生成语句的连贯性及与参考语句的词汇重叠情况。</li>
<li><strong>多样性</strong>：使用 Distinct-n（n=1, 2）来衡量生成回复的多样性。</li>
<li><strong>人工评估</strong>：从相关性、流畅性与信息量对生成的语句进行评价，评分范围[0,2]。对100条随机选取、对话轮数超过20轮的样本进行了评估。Fleiss Kappa 值为 0.72</li>
</ol>
<h2 id="结果">结果</h2>
<p>模型 <strong>CPD</strong> 在缓解位置偏差并增强因果感知能力方面表现最优。</p>
<p>长期对话方法通过摘要和检索压缩对话历史，从而在一定程度上缓解了位置偏差问题。</p>
<p>在对话轮数较短的 ESConv 数据集中，基于摘要的 RSM 优于基于检索的 CONSTRAIN；而在轮数较长的 MSC 数据集中，CONSTRAIN 表现更好。</p>
<p>CONSTRAIN 在多样性指标上表现较差，说明其未能充分利用对话信息。</p>
<p><img src="/images/image-20250609002216930.png" alt="image-20250609002216930"></p>
<h2 id="消融实验">消融实验</h2>
<p>为了验证我们所提出方法的有效性，作者建立了三个变体：</p>
<ol>
<li><strong>w/o IRM</strong>：去除不变风险最小化损失；</li>
<li><strong>w/o MTE</strong>：去除最大化处理效应损失；</li>
<li><strong>w/o sampling</strong>：去除位置差异采样策略；</li>
</ol>
<p><img src="/images/image-20250609002236578.png" alt="image-20250609002236578"></p>
<p>所有消融变体在不同程度上都出现了性能下降，进一步验证了我们方法中各组成部分的有效性。其中，不变风险最小化损失能够有效引导模型生成更接近黄金回复的内容，而最大化处理效应损失则增强了模型生成回复的多样性。去除采样策略后的性能显著下降，说明该策略能有效缓解位置不平衡问题。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/causal-debias/">causal debias</a></div><div class="post-share"><div class="social-share" data-image="/img/Hazel.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav id="pagination"><div class="pagination"></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/06/11/Causal-Estimation/" title="Causal Estimation of Memorisation Profiles"><img class="cover" src="/img/Hazel.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-11</div><div class="info-item-2">Causal Estimation of Memorisation Profiles</div></div><div class="info-2"><div class="info-item-1">   收录于 ACL 2024 best papers             ...</div></div></div></a><a class="pagination-related" href="/2025/06/09/Gain-Guided/" title="Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models"><img class="cover" src="/img/Hazel.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-09</div><div class="info-item-2">Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models</div></div><div class="info-2"><div class="info-item-1">   收录于 arxiv                Abstract 本文主要工作： 提出信息增益引导的因果干预去偏框架（information gain-guided causal intervention debiasing,ICD），为消除指令微调数据集中的偏差，必须确保这些偏差特征对预测答案不提供任何附加信息，即令偏差特征的信息增益为0 。框架利用基于因果干预的数据重写方法，自动并自主地平衡指令微调数据集的分布，降低信息增益。在去偏后的数据集上采用标准监督微调流程训练LLM。 Dataset：		 Llama3.1-8B、部分 Flan 2021、MMLU 、BBH 、TruthfulQA Baseline：	       Vanilla SFT、Razor LLM:    		    Llama3.1-8B     Gemma-9B 评估方法:		In-Domain Test Sets、Transfer Test Sets、Challenge Test...</div></div></div></a><a class="pagination-related" href="/2025/06/10/cal/" title="Causal-Guided Active Learning for Debiasing Large Language Models"><img class="cover" src="/img/Hazel.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-10</div><div class="info-item-2">Causal-Guided Active Learning for Debiasing Large Language Models</div></div><div class="info-2"><div class="info-item-1">   收录于 ACL                Abstract 作者提出了一种因果引导的主动学习框架（Causal-Guided Active Learning，CAL），框架利用LLM自身，自动且自主的识别信息量丰富的的偏见样本并归纳偏见模式，随后采用一种高效且低成本的基于上下文学习（ICL）方法，在生成过程中防止LLM利用数据集偏见。 主动学习旨在选择最具信息量的样本，并查询外部信息源对其标注。在去偏场景中，CAL通过寻找模型无法建模因果不变语义关系的样本来识别偏见实例，然后通过发现数据集偏见对LLM生成影响最大的样本，选取最具信息量的偏见实例。 Dataset: Chatbot、MT-Bench...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/1749020983551.gif" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Anifled</div><div class="author-info-description">我与我 周旋久</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/07/12/implicit-bias/" title="Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions"><img src="/img/Hazel.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions"/></a><div class="content"><a class="title" href="/2025/07/12/implicit-bias/" title="Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions">Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions</a><time datetime="2025-07-12T06:58:37.000Z" title="发表于 2025-07-12 14:58:37">2025-07-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/11/idobata/" title="A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing"><img src="/img/Hazel.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing"/></a><div class="content"><a class="title" href="/2025/07/11/idobata/" title="A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing">A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing</a><time datetime="2025-07-11T13:06:50.000Z" title="发表于 2025-07-11 21:06:50">2025-07-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/10/mamo/" title="Mitigating Social Bias in Large Language Models: A Multi-Objective Approach Within a Multi-Agent Framework"><img src="/img/Hazel.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Mitigating Social Bias in Large Language Models: A Multi-Objective Approach Within a Multi-Agent Framework"/></a><div class="content"><a class="title" href="/2025/07/10/mamo/" title="Mitigating Social Bias in Large Language Models: A Multi-Objective Approach Within a Multi-Agent Framework">Mitigating Social Bias in Large Language Models: A Multi-Objective Approach Within a Multi-Agent Framework</a><time datetime="2025-07-10T03:17:05.000Z" title="发表于 2025-07-10 11:17:05">2025-07-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/24/ai-search/" title="Towards AI Search Paradigm"><img src="/img/Hazel.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Towards AI Search Paradigm"/></a><div class="content"><a class="title" href="/2025/06/24/ai-search/" title="Towards AI Search Paradigm">Towards AI Search Paradigm</a><time datetime="2025-06-24T11:47:28.000Z" title="发表于 2025-06-24 19:47:28">2025-06-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/23/RAGentA/" title="RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed Question Answering"><img src="/img/Hazel.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed Question Answering"/></a><div class="content"><a class="title" href="/2025/06/23/RAGentA/" title="RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed Question Answering">RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed Question Answering</a><time datetime="2025-06-23T09:17:16.000Z" title="发表于 2025-06-23 17:17:16">2025-06-23</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Java/"><span class="card-category-list-name">Java</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Papers/"><span class="card-category-list-name">Papers</span><span class="card-category-list-count">9</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/Java/" style="font-size: 1.1em; color: #999">Java</a> <a href="/tags/causal-debias/" style="font-size: 1.5em; color: #99a9bf">causal debias</a> <a href="/tags/multi-agent/" style="font-size: 1.37em; color: #99a4b2">multi agent</a> <a href="/tags/multi-agent/" style="font-size: 1.23em; color: #999ea6">multi-agent</a> <a href="/tags/%E5%85%A5%E9%97%A8/" style="font-size: 1.1em; color: #999">入门</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/07/">
            <span class="card-archive-list-date">
              七月 2025
            </span>
            <span class="card-archive-list-count">3</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/06/">
            <span class="card-archive-list-date">
              六月 2025
            </span>
            <span class="card-archive-list-count">7</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">10</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-07-12T09:05:12.250Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-color: rgb(219,255,253);"><div id="footer-wrap"><div class="copyright">&copy;2025 By Anifled</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><div class="footer_custom_text">作者：<strong>anifled</strong>，转载请注明作者。<a target="_blank" rel="noopener" href="https://github.com/anifled12138">GitHub</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>